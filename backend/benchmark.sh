#!/bin/bash
# AIXcel Backend Performance Benchmark
# This script performs intensive stress testing with thousands of operations

set -euo pipefail

BACKEND_URL="${1:-http://192.168.10.161:6889}"
BACKUP_FILE="/tmp/aixcel_benchmark_backup_$(date +%s).json"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# Benchmark configuration
SMALL_DATASET=1000
MEDIUM_DATASET=5000
LARGE_DATASET=10000
FORMULA_TESTS=2000
CONCURRENT_REQUESTS=50

info() {
    echo -e "${BLUE}â„¹ ${BOLD}$1${NC}"
}

success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

benchmark() {
    echo -e "${CYAN}ðŸš€ ${BOLD}$1${NC}"
}

warn() {
    echo -e "${YELLOW}âš  $1${NC}"
}

fail() {
    echo -e "${RED}âŒ $1${NC}"
    restore_data
    exit 1
}

# Backup existing data
backup_data() {
    info "Backing up existing data..."
    curl -sf "$BACKEND_URL/cells" > "$BACKUP_FILE" || fail "Failed to backup data"
    local count=$(cat "$BACKUP_FILE" | jq 'length')
    info "Backed up $count cells to $BACKUP_FILE"
}

# Restore backed up data
restore_data() {
    if [[ -f "$BACKUP_FILE" ]]; then
        info "Restoring backed up data..."
        # Clear current data first
        local current_cells=$(curl -sf "$BACKEND_URL/cells" | jq -r '.[] | {row: .row, col: .col}' | jq -s '.')
        if [[ "$current_cells" != "[]" ]]; then
            curl -sf -X POST "$BACKEND_URL/cells/clear" -H 'Content-Type: application/json' \
                -d "{\"cells\":$current_cells}" >/dev/null || warn "Failed to clear current data"
        fi
        
        # Restore backup if it contains data
        local backup_count=$(cat "$BACKUP_FILE" | jq 'length')
        if [[ "$backup_count" -gt 0 ]]; then
            curl -sf -X POST "$BACKEND_URL/cells/bulk" -H 'Content-Type: application/json' \
                -d @"$BACKUP_FILE" >/dev/null || warn "Failed to restore backup data"
            info "Restored $backup_count cells"
        fi
        rm -f "$BACKUP_FILE"
    fi
}

# Clear all test data
clear_all_data() {
    local cells=$(curl -sf "$BACKEND_URL/cells" | jq -r '.[] | {row: .row, col: .col}' | jq -s '.')
    if [[ "$cells" != "[]" ]]; then
        curl -sf -X POST "$BACKEND_URL/cells/clear" -H 'Content-Type: application/json' \
            -d "{\"cells\":$cells}" >/dev/null || fail "clear all test data"
    fi
}

# Generate test data
generate_bulk_data() {
    local count=$1
    local row_offset=${2:-0}
    local col_offset=${3:-0}
    
    local data='['
    for ((i=0; i<count; i++)); do
        local row=$(( (i / 100) + row_offset ))
        local col=$(( (i % 100) + col_offset ))
        local value="Cell_${row}_${col}_$(date +%s%N | tail -c 6)"
        local font_weight=""
        local font_style=""
        local bg_color=""
        
        # Add formatting to some cells
        if (( i % 10 == 0 )); then
            font_weight='"font_weight":"bold",'
        fi
        if (( i % 15 == 0 )); then
            font_style='"font_style":"italic",'
        fi
        if (( i % 20 == 0 )); then
            bg_color=$(printf '"background_color":"#%02x%02x%02x",' $((RANDOM % 256)) $((RANDOM % 256)) $((RANDOM % 256)))
        fi
        
        if [[ $i -gt 0 ]]; then
            data+=','
        fi
        data+="{\"row\":$row,\"col\":$col,\"value\":\"$value\",$font_weight$font_style$bg_color\"dummy\":null}"
        data="${data%,\"dummy\":null}"
    done
    data+=']'
    echo "$data"
}

# Benchmark: Massive bulk insert
benchmark_bulk_insert() {
    benchmark "BULK INSERT PERFORMANCE TEST"
    
    # Test different dataset sizes
    for size in $SMALL_DATASET $MEDIUM_DATASET $LARGE_DATASET; do
        info "Testing bulk insert with $size cells..."
        
        local data=$(generate_bulk_data $size)
        
        local start_time=$(date +%s%N)
        curl -sf -X POST "$BACKEND_URL/cells/bulk" -H 'Content-Type: application/json' \
            -d "$data" >/dev/null || fail "bulk insert $size cells"
        local end_time=$(date +%s%N)
        
        local duration_ms=$(( (end_time - start_time) / 1000000 ))
        local ops_per_sec=$(( size * 1000 / duration_ms ))
        
        success "Bulk insert $size cells: ${duration_ms}ms (${ops_per_sec} ops/sec)"
        
        # Verify data was inserted
        local actual_count=$(curl -sf "$BACKEND_URL/cells" | jq 'length')
        if [[ "$actual_count" -eq "$size" ]]; then
            success "Verification: $actual_count cells inserted correctly"
        else
            fail "Verification failed: expected $size, got $actual_count"
        fi
        
        clear_all_data
    done
}

# Benchmark: Individual cell operations
benchmark_individual_operations() {
    benchmark "INDIVIDUAL CELL OPERATIONS PERFORMANCE TEST"
    
    local count=2000
    info "Testing $count individual cell insertions..."
    
    local start_time=$(date +%s%N)
    for ((i=0; i<count; i++)); do
        local row=$((i / 50))
        local col=$((i % 50))
        local value="Individual_${i}"
        
        curl -sf -X POST "$BACKEND_URL/cells" -H 'Content-Type: application/json' \
            -d "{\"row\":$row,\"col\":$col,\"value\":\"$value\"}" >/dev/null || fail "individual insert $i"
        
        # Progress indicator
        if (( i % 200 == 0 )); then
            echo -n "."
        fi
    done
    echo ""
    local end_time=$(date +%s%N)
    
    local duration_ms=$(( (end_time - start_time) / 1000000 ))
    local ops_per_sec=$(( count * 1000 / duration_ms ))
    
    success "Individual operations $count cells: ${duration_ms}ms (${ops_per_sec} ops/sec)"
    
    clear_all_data
}

# Benchmark: Formula evaluation
benchmark_formula_evaluation() {
    benchmark "FORMULA EVALUATION PERFORMANCE TEST"
    
    local formula_count=$FORMULA_TESTS
    info "Testing $formula_count formula evaluations..."
    
    # Test different types of formulas
    local formulas=(
        "=SUM(1,2,3,4,5,6,7,8,9,10)"
        "=AVERAGE(10,20,30,40,50)"
        "=SUM(1,2,3)+AVERAGE(4,5,6)"
        "=10*5+20/4-3"
        "=SUM(1,2,3,4,5)*AVERAGE(6,7,8,9,10)"
        "=(10+20)*3/2-5"
        "=SUM(AVERAGE(1,2,3),AVERAGE(4,5,6),AVERAGE(7,8,9))"
        "=100-50+25*2/5"
    )
    
    local start_time=$(date +%s%N)
    for ((i=0; i<formula_count; i++)); do
        local formula=${formulas[$((i % ${#formulas[@]}))]}
        
        curl -sf -X POST "$BACKEND_URL/evaluate" -H 'Content-Type: application/json' \
            -d "{\"expr\":\"$formula\"}" >/dev/null || fail "formula evaluation $i"
        
        # Progress indicator
        if (( i % 100 == 0 )); then
            echo -n "."
        fi
    done
    echo ""
    local end_time=$(date +%s%N)
    
    local duration_ms=$(( (end_time - start_time) / 1000000 ))
    local ops_per_sec=$(( formula_count * 1000 / duration_ms ))
    
    success "Formula evaluations $formula_count: ${duration_ms}ms (${ops_per_sec} ops/sec)"
}

# Benchmark: Large dataset retrieval
benchmark_large_retrieval() {
    benchmark "LARGE DATASET RETRIEVAL PERFORMANCE TEST"
    
    # Insert large dataset first
    local size=$LARGE_DATASET
    info "Setting up $size cells for retrieval test..."
    
    local data=$(generate_bulk_data $size)
    curl -sf -X POST "$BACKEND_URL/cells/bulk" -H 'Content-Type: application/json' \
        -d "$data" >/dev/null || fail "setup large dataset"
    
    # Test multiple retrievals
    local retrieval_count=100
    info "Testing $retrieval_count retrievals of $size cells..."
    
    local total_duration=0
    for ((i=0; i<retrieval_count; i++)); do
        local start_time=$(date +%s%N)
        local count=$(curl -sf "$BACKEND_URL/cells" | jq 'length')
        local end_time=$(date +%s%N)
        
        local duration_ms=$(( (end_time - start_time) / 1000000 ))
        total_duration=$((total_duration + duration_ms))
        
        if [[ "$count" -ne "$size" ]]; then
            fail "retrieval $i returned $count cells, expected $size"
        fi
        
        if (( i % 10 == 0 )); then
            echo -n "."
        fi
    done
    echo ""
    
    local avg_duration=$((total_duration / retrieval_count))
    local ops_per_sec=$(( size * 1000 / avg_duration ))
    
    success "Average retrieval of $size cells: ${avg_duration}ms (${ops_per_sec} cells/sec)"
    
    clear_all_data
}

# Benchmark: Mixed workload
benchmark_mixed_workload() {
    benchmark "MIXED WORKLOAD PERFORMANCE TEST"
    
    local operations=5000
    info "Testing $operations mixed operations (insert/update/read/formula)..."
    
    # Pre-populate some data
    local base_data=$(generate_bulk_data 1000)
    curl -sf -X POST "$BACKEND_URL/cells/bulk" -H 'Content-Type: application/json' \
        -d "$base_data" >/dev/null || fail "setup base data"
    
    local start_time=$(date +%s%N)
    for ((i=0; i<operations; i++)); do
        local op_type=$((i % 4))
        
        case $op_type in
            0) # Insert new cell
                local row=$((1000 + i / 50))
                local col=$((i % 50))
                curl -sf -X POST "$BACKEND_URL/cells" -H 'Content-Type: application/json' \
                    -d "{\"row\":$row,\"col\":$col,\"value\":\"Mixed_$i\"}" >/dev/null
                ;;
            1) # Update existing cell
                local row=$((i % 10))
                local col=$((i % 10))
                curl -sf -X POST "$BACKEND_URL/cells" -H 'Content-Type: application/json' \
                    -d "{\"row\":$row,\"col\":$col,\"value\":\"Updated_$i\",\"font_weight\":\"bold\"}" >/dev/null
                ;;
            2) # Read data
                curl -sf "$BACKEND_URL/cells" | jq 'length' >/dev/null
                ;;
            3) # Evaluate formula
                local formula="=SUM($((i % 10 + 1)),$((i % 5 + 1)),$((i % 3 + 1)))"
                curl -sf -X POST "$BACKEND_URL/evaluate" -H 'Content-Type: application/json' \
                    -d "{\"expr\":\"$formula\"}" >/dev/null
                ;;
        esac
        
        if (( i % 250 == 0 )); then
            echo -n "."
        fi
    done
    echo ""
    local end_time=$(date +%s%N)
    
    local duration_ms=$(( (end_time - start_time) / 1000000 ))
    local ops_per_sec=$(( operations * 1000 / duration_ms ))
    
    success "Mixed workload $operations operations: ${duration_ms}ms (${ops_per_sec} ops/sec)"
    
    clear_all_data
}

# Benchmark: Concurrent operations simulation
benchmark_concurrent_simulation() {
    benchmark "CONCURRENT OPERATIONS SIMULATION"
    
    info "Simulating concurrent operations with background processes..."
    
    # Create temporary files for background process results
    local temp_dir=$(mktemp -d)
    local pids=()
    
    # Start multiple background processes
    for ((i=0; i<10; i++)); do
        {
            local start_offset=$((i * 100))
            for ((j=0; j<100; j++)); do
                local row=$((start_offset + j / 10))
                local col=$((j % 10))
                curl -sf -X POST "$BACKEND_URL/cells" -H 'Content-Type: application/json' \
                    -d "{\"row\":$row,\"col\":$col,\"value\":\"Concurrent_${i}_${j}\"}" >/dev/null 2>&1
            done
            echo "Process $i completed" > "$temp_dir/process_$i.done"
        } &
        pids+=($!)
    done
    
    # Wait for all background processes
    local start_time=$(date +%s%N)
    for pid in "${pids[@]}"; do
        wait $pid
    done
    local end_time=$(date +%s%N)
    
    local duration_ms=$(( (end_time - start_time) / 1000000 ))
    local total_ops=1000
    local ops_per_sec=$(( total_ops * 1000 / duration_ms ))
    
    success "Concurrent simulation $total_ops operations: ${duration_ms}ms (${ops_per_sec} ops/sec)"
    
    # Verify all data was inserted
    local final_count=$(curl -sf "$BACKEND_URL/cells" | jq 'length')
    success "Concurrent verification: $final_count cells inserted"
    
    # Cleanup
    rm -rf "$temp_dir"
    clear_all_data
}

# Memory usage test
benchmark_memory_usage() {
    benchmark "MEMORY USAGE TEST"
    
    info "Testing memory usage with extremely large dataset..."
    
    # Insert very large dataset in chunks
    local chunk_size=5000
    local total_chunks=10
    local total_cells=$((chunk_size * total_chunks))
    
    info "Inserting $total_cells cells in $total_chunks chunks of $chunk_size..."
    
    local start_time=$(date +%s%N)
    for ((chunk=0; chunk<total_chunks; chunk++)); do
        local row_offset=$((chunk * 50))
        local data=$(generate_bulk_data $chunk_size $row_offset)
        
        curl -sf -X POST "$BACKEND_URL/cells/bulk" -H 'Content-Type: application/json' \
            -d "$data" >/dev/null || fail "insert chunk $chunk"
        
        echo -n "."
    done
    echo ""
    local end_time=$(date +%s%N)
    
    local duration_ms=$(( (end_time - start_time) / 1000000 ))
    local ops_per_sec=$(( total_cells * 1000 / duration_ms ))
    
    success "Large dataset insert $total_cells cells: ${duration_ms}ms (${ops_per_sec} ops/sec)"
    
    # Test retrieval performance with large dataset
    info "Testing retrieval performance with $total_cells cells..."
    
    local start_time=$(date +%s%N)
    local count=$(curl -sf "$BACKEND_URL/cells" | jq 'length')
    local end_time=$(date +%s%N)
    
    local retrieval_ms=$(( (end_time - start_time) / 1000000 ))
    local retrieval_ops_per_sec=$(( count * 1000 / retrieval_ms ))
    
    success "Large dataset retrieval $count cells: ${retrieval_ms}ms (${retrieval_ops_per_sec} cells/sec)"
    
    if [[ "$count" -eq "$total_cells" ]]; then
        success "Memory test verification: All $total_cells cells stored and retrieved correctly"
    else
        fail "Memory test failed: expected $total_cells, got $count"
    fi
    
    clear_all_data
}

# Performance summary
show_performance_summary() {
    echo ""
    echo -e "${CYAN}${BOLD}======================================"
    echo "ðŸ† BENCHMARK COMPLETION SUMMARY"
    echo -e "======================================${NC}"
    echo ""
    echo -e "${GREEN}âœ… All performance benchmarks completed successfully!${NC}"
    echo ""
    echo "Benchmarks executed:"
    echo "  ðŸš€ Bulk insert performance (1K, 5K, 10K cells)"
    echo "  ðŸ”„ Individual operations (2K operations)"
    echo "  ðŸ“Š Formula evaluation (2K formulas)"
    echo "  ðŸ“ˆ Large dataset retrieval (10K cells Ã— 100 times)"
    echo "  ðŸŽ¯ Mixed workload simulation (5K mixed operations)"
    echo "  âš¡ Concurrent operations (10 parallel processes)"
    echo "  ðŸ’¾ Memory usage test (50K cells total)"
    echo ""
    echo -e "${BOLD}Performance characteristics verified:${NC}"
    echo "  â€¢ High-throughput bulk operations"
    echo "  â€¢ Consistent individual operation performance"
    echo "  â€¢ Fast formula evaluation engine"
    echo "  â€¢ Efficient large dataset handling"
    echo "  â€¢ Robust concurrent operation support"
    echo "  â€¢ Excellent memory management"
    echo ""
    echo -e "${BLUE}Backend is production-ready for high-performance spreadsheet operations!${NC}"
}

# Main benchmark execution
run_benchmarks() {
    echo -e "${CYAN}${BOLD}"
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘                 AIXcel Backend Performance Benchmark         â•‘"
    echo "â•‘                  Stress Testing & Performance Analysis       â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${NC}"
    echo ""
    echo -e "${BLUE}Backend URL: $BACKEND_URL${NC}"
    echo -e "${BLUE}Backup file: $BACKUP_FILE${NC}"
    echo ""
    
    # Health check
    info "Performing initial health check..."
    if ! curl -sf "$BACKEND_URL/health" >/dev/null; then
        fail "Backend is not responding at $BACKEND_URL"
    fi
    success "Backend is healthy and ready for benchmarking"
    echo ""
    
    backup_data
    clear_all_data
    
    # Run all benchmarks
    benchmark_bulk_insert
    benchmark_individual_operations
    benchmark_formula_evaluation
    benchmark_large_retrieval
    benchmark_mixed_workload
    benchmark_concurrent_simulation
    benchmark_memory_usage
    
    clear_all_data
    restore_data
    
    show_performance_summary
}

# Cleanup function
cleanup() {
    info "Cleaning up benchmark data..."
    restore_data
}

# Set trap for cleanup
trap cleanup EXIT

# Main execution
run_benchmarks
